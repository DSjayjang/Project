{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd04c03b-ca22-4b3f-913a-97754d9d99ca",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e5afb65-1990-4528-ada1-fb762ca4d97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Programming\\Anaconda\\envs\\color\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Programming\\Anaconda\\envs\\color\\lib\\site-packages\\controlnet_aux\\mediapipe_face\\mediapipe_face_common.py:7: UserWarning: The module 'mediapipe' is not installed. The package will have limited functionality. Please install it using the command: pip install 'mediapipe'\n",
      "  warnings.warn(\n",
      "c:\\Programming\\Anaconda\\envs\\color\\lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "c:\\Programming\\Anaconda\\envs\\color\\lib\\site-packages\\timm\\models\\registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "c:\\Programming\\Anaconda\\envs\\color\\lib\\site-packages\\controlnet_aux\\segment_anything\\modeling\\tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_5m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_5m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "c:\\Programming\\Anaconda\\envs\\color\\lib\\site-packages\\controlnet_aux\\segment_anything\\modeling\\tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_11m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_11m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "c:\\Programming\\Anaconda\\envs\\color\\lib\\site-packages\\controlnet_aux\\segment_anything\\modeling\\tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "c:\\Programming\\Anaconda\\envs\\color\\lib\\site-packages\\controlnet_aux\\segment_anything\\modeling\\tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_384 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "c:\\Programming\\Anaconda\\envs\\color\\lib\\site-packages\\controlnet_aux\\segment_anything\\modeling\\tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_512 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "import zipfile\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "from controlnet_aux import CannyDetector\n",
    "import open_clip\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d00d1431-99c6-4d85-8d1d-5a2ffea97d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd0df5c-8f4c-403b-bcfb-3c21f5fd3fbf",
   "metadata": {},
   "source": [
    "## Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62d41dfb-a31a-4545-b2d6-c3ecef7657a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'SUB_DIR' : './submission',\n",
    "    'SEED' : 42\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4c8772-b25f-44c8-9d29-1be79dc60155",
   "metadata": {},
   "source": [
    "## Fixed RandomSeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f52d96c7-d14a-40fe-93b2-5388f94bdce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65a27d7-c439-4439-86c8-441ead77f5f8",
   "metadata": {},
   "source": [
    "## Load Pre-trained Model (Stable-Diffusion-V1-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0dac3df-dedd-4304-814c-941ad26d6467",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 15 files: 100%|██████████| 15/15 [01:06<00:00,  4.44s/it]\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:01<00:00,  4.55it/s]\n"
     ]
    }
   ],
   "source": [
    "controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16).to(device)\n",
    "\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16\n",
    ").to(device)\n",
    "\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config) # 스케줄러 최적화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1e69df-2965-4fc8-8282-5f1740fe13fc",
   "metadata": {},
   "source": [
    "## Pre-Processing Input Image (Controlnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a74583d-c618-44ee-9640-f9601e6c67ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_controlnet(image: Image.Image, detector_type: str = \"canny\") -> Image.Image:\n",
    "    if detector_type == \"canny\":\n",
    "        canny_detector = CannyDetector()\n",
    "        image_np = np.array(image)\n",
    "        control_image_np = canny_detector(image_np)\n",
    "        return Image.fromarray(control_image_np)\n",
    "    elif detector_type == \"hed\":\n",
    "        hed_detector = HEDdetector.from_pretrained('lllyasviel/Annotator').to(device)\n",
    "        image_np = np.array(image)\n",
    "        control_image_np = hed_detector(image_np)\n",
    "        return Image.fromarray(control_image_np)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported detector_type. Choose 'canny' or 'hed'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deca6e3a-3095-4f06-9a8e-b657d76e6b1e",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd13f422-2104-4f7b-886c-ae98e5ae9569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>input_img_path</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_001</td>\n",
       "      <td>./test/input_image/TEST_001.png</td>\n",
       "      <td>what is the item of furniture that is to the l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_002</td>\n",
       "      <td>./test/input_image/TEST_002.png</td>\n",
       "      <td>person wearing white hat. do you see any pans ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_003</td>\n",
       "      <td>./test/input_image/TEST_003.png</td>\n",
       "      <td>in this image i can see few persons standing. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_004</td>\n",
       "      <td>./test/input_image/TEST_004.png</td>\n",
       "      <td>a girl lying on a bench. dark skin man sitting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_005</td>\n",
       "      <td>./test/input_image/TEST_005.png</td>\n",
       "      <td>dog standing on a car. steering wheel in the t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>TEST_196</td>\n",
       "      <td>./test/input_image/TEST_196.png</td>\n",
       "      <td>woman and child wearing ski goggles. two hands...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>TEST_197</td>\n",
       "      <td>./test/input_image/TEST_197.png</td>\n",
       "      <td>the bear is white. black nose of bear. green g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>TEST_198</td>\n",
       "      <td>./test/input_image/TEST_198.png</td>\n",
       "      <td>man wearing a white shirt. what is the vehicle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>TEST_199</td>\n",
       "      <td>./test/input_image/TEST_199.png</td>\n",
       "      <td>set of wedding invitations and cards laid out ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>TEST_200</td>\n",
       "      <td>./test/input_image/TEST_200.png</td>\n",
       "      <td>what device is to the left of the monkey? what...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                   input_img_path  \\\n",
       "0    TEST_001  ./test/input_image/TEST_001.png   \n",
       "1    TEST_002  ./test/input_image/TEST_002.png   \n",
       "2    TEST_003  ./test/input_image/TEST_003.png   \n",
       "3    TEST_004  ./test/input_image/TEST_004.png   \n",
       "4    TEST_005  ./test/input_image/TEST_005.png   \n",
       "..        ...                              ...   \n",
       "195  TEST_196  ./test/input_image/TEST_196.png   \n",
       "196  TEST_197  ./test/input_image/TEST_197.png   \n",
       "197  TEST_198  ./test/input_image/TEST_198.png   \n",
       "198  TEST_199  ./test/input_image/TEST_199.png   \n",
       "199  TEST_200  ./test/input_image/TEST_200.png   \n",
       "\n",
       "                                               caption  \n",
       "0    what is the item of furniture that is to the l...  \n",
       "1    person wearing white hat. do you see any pans ...  \n",
       "2    in this image i can see few persons standing. ...  \n",
       "3    a girl lying on a bench. dark skin man sitting...  \n",
       "4    dog standing on a car. steering wheel in the t...  \n",
       "..                                                 ...  \n",
       "195  woman and child wearing ski goggles. two hands...  \n",
       "196  the bear is white. black nose of bear. green g...  \n",
       "197  man wearing a white shirt. what is the vehicle...  \n",
       "198  set of wedding invitations and cards laid out ...  \n",
       "199  what device is to the left of the monkey? what...  \n",
       "\n",
       "[200 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(r'C:\\Users\\user\\Desktop\\연구\\6. Colorization') # pcrl\n",
    "\n",
    "test_df = pd.read_csv('./test.csv')\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44b59ada-90b9-4e23-ac53-a8ae5b9360b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (78 > 77). Running this sequence through the model will result in indexing errors\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "100%|██████████| 50/50 [00:06<00:00,  8.32it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.22it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.23it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['bench .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.20it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['light of an suv .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.10it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['. a light blue t - shirt .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.01it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.11it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['brown hair .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.14it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.12it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['in the back there is a box in white color . to the side of that there is a rack and sum objects in it . there is a red color wall .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.07it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.94it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.14it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['tire .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.11it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['this is tree .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.00it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.06it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.91it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of a hamster .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.11it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.23it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['. on the right side , we see the trees . on the left side , we see the buildings and the railing . there are trees and buildings in the background . at the bottom , we see the road .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.25it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.30it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['color .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.13it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.02it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['clouds .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.01it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.14it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.10it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.15it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.00it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.00it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.74it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.68it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.89it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.01it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.14it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.21it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.20it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['are black chairs . outside the building there are trees .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.15it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.99it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.14it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.16it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and wearing a red color t - shirt sitting on the chair and a man wearing a grey color shirt is smiling and putting hand on his head . in front on the table we can see many bottles , creams and some brushes .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.10it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['is spotted . this is an ear .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.09it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.06it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.08it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.13it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.18it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.19it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.86it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.99it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.26it/s]\n",
      "Potential NSFW content was detected in one or more images. A black image will be returned instead. Try again with a different prompt and/or seed.\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.21it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['green wall . the tennis net .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.17it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.19it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.04it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.02it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.96it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the bib . short fuzzy hair .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.82it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.02it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['is bright pink .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.97it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['her , there are two legs of another person on the mat . in the background , there is a black board , a basket on a sewing machine , there are trees and other objects .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.07it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['. i can also see the dog to the side of the person which is in white color . the person and the dog are on the platform']\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.98it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.13it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.14it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['man standing wearing a black colour dress and a white colour hat is smiling and the woman standing beside the man is smiling . on the left side there is a hand of the person holding a mobile phone which is visible .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.91it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['. part of a hand .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.03it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.00it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.02it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['roof .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.17it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['sandal . sun rays on wooden floor .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.98it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.93it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.06it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['cking signs .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.98it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.18it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.16it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.93it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['is written on it . and there is also the railing . in the background there are many trees and also the sky .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.14it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['snow .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.25it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.09it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.18it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.92it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.87it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['blue drapes .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.83it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['blanket . left ear on cat .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.97it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['color trees , in the top there is a sky which is in blue color .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.32it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.08it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"'s white undershirt .\"]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.19it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.27it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.97it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['. person at the middle of image is wearing white colour sports dress is on snow']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.26it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['car . letter o on car .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.12it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.04it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.24it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.23it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.25it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.12it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.99it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.10it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.13it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.88it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.19it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.00it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['color .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.09it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.06it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.14it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['p .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.31it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['clouds .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.29it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', there is a net in between the two persons .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.94it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['pants are black .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.18it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.30it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.29it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.08it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.83it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['no left turn sign . human .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.87it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['in grey color .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.19it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['an empty bread bowl .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.93it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['white color .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.24it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['see the building , windows with a brown roof top . in the background we can see the sky , trees and buildings .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.31it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.22it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['. sign on a pole .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.35it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.32it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.99it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['trees in the background and some grass on the ground . at the left side the persons are visible .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.03it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.96it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.06it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['bus seat .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.00it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.04it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['bowls .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.12it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.33it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['. brown trim on sofa .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.36it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.17it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['is in white color .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.30it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of a man .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.31it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.32it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.32it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.29it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.09it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.20it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.20it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.00it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.31it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a white cap .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.12it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['an object which is black in colour . on the right side there is a window and there is a table . on the table there is a bowl which is white in colour . on the left side there is a stove , on the stove there is a kettle .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.24it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a red and white sign . a tall , leafless tree .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.29it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.22it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.93it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.09it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.05it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['pellar .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.31it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['blind .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.23it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a big black cow .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.13it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.27it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.29it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['is white in color .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.27it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['blue ski .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.90it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.83it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['shirt . a black van .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.02it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.32it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', in the left side there is a white color flush box and there is a white color wash basin , in the background there is a white color wall and there is a picture on the wall .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.15it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.20it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.28it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.28it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.23it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.33it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['over here .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.21it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.11it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of white bowl .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.33it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.30it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['. there is a cupboard to the wall .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.18it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.28it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.30it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.25it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', a black colored plate and a cardboard box and on the cardboard box i can see a cake . in the background i can see the wall , a photo frame attached to the wall , the window and the curtain .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.26it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.22it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['indoors ?']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.27it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.31it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.17it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.03it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.21it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.28it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['ground and in the background i can see a building , few trees and the sky .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.12it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.20it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.28it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.19it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.22it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.15it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.12it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.20it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.99it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['see the floor . we see a metal chain in red color . on the right side , we see the barrier poles and the objects in red and black color . beside that , we see a pillar . in the background , we see a wall and a door .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.10it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.04it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.12it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['shirt . yellow boots on the man .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.13it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.00it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.25it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['is casting a shadow .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.30it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.17it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.08it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['tan bear . ears of a tan bear .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.18it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['car lights .']\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.31it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.29it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test 데이터셋에 대한 모든 이미지 생성 완료.\n"
     ]
    }
   ],
   "source": [
    "out_imgs = []\n",
    "out_img_names = []\n",
    "for img_id, img_path, caption in zip(test_df['ID'], test_df['input_img_path'], test_df['caption']):\n",
    "    input_img  = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    control_image = preprocess_for_controlnet(input_img, detector_type=\"canny\")\n",
    "\n",
    "    full_prompt = f\"realistic, high quality, detailed, Do not change the structure. Only Colorize. {caption}\"\n",
    "    output_img = pipe(\n",
    "            prompt=full_prompt,\n",
    "            image=control_image, # ControlNet에 의해 전처리된 이미지\n",
    "            guidance_scale=7.5, # 텍스트 프롬프트 충실도\n",
    "            num_inference_steps=50, # 생성 스텝 수\n",
    "        ).images[0]\n",
    "\n",
    "    out_imgs.append(output_img)\n",
    "    out_img_names.append(img_id)\n",
    "print('✅ Test 데이터셋에 대한 모든 이미지 생성 완료.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4002aa-82c2-4183-bf36-10083e78583d",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0b7ab55-1d73-4e2e-8d83-018c367672b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론 결과물 디렉토리 생성\n",
    "os.makedirs(CFG['SUB_DIR'], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5683c6c9-2d8d-4038-aac6-2b890430bfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **중요** 추론 이미지 평가용 Embedding 추출 모델\n",
    "clip_model, _, clip_preprocess = open_clip.create_model_and_transforms(\"ViT-L-14\", pretrained=\"openai\") # 모델명을 반드시 일치시켜야합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "114f0e21-b487-4aa0-8fa5-316d90b71c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:14, 13.35it/s]\n"
     ]
    }
   ],
   "source": [
    "clip_model.to(device)\n",
    "# 평가 제출을 위해 추론된 이미지들을 ViT-L-14 모델로 임베딩 벡터(Feature)를 추출합니다.\n",
    "feat_imgs = []\n",
    "for output_img, img_id in tqdm(zip(out_imgs, out_img_names)):\n",
    "    path_out_img = CFG['SUB_DIR'] + '/' + img_id + '.png' \n",
    "    output_img.save(path_out_img)\n",
    "    # 평가용 임베딩 생성 및 저장\n",
    "    output_img = clip_preprocess(output_img).unsqueeze(0).cuda()\n",
    "    with torch.no_grad():\n",
    "        feat_img = clip_model.encode_image(output_img)\n",
    "        feat_img /= feat_img.norm(dim=-1, keepdim=True) # L2 정규화 필수\n",
    "\n",
    "    feat_img = feat_img.detach().cpu().numpy().reshape(-1)\n",
    "    feat_imgs.append(feat_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4dc3fb0f-ff04-4fa1-8dd5-576f386a61fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imgs = np.array(feat_imgs)\n",
    "vec_columns = [f'vec_{i}' for i in range(feat_imgs.shape[1])]\n",
    "feat_submission = pd.DataFrame(feat_imgs, columns=vec_columns)\n",
    "feat_submission.insert(0, 'ID', out_img_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9633b3f8-3034-4448-aac5-2e9002e30ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_submission.to_csv(CFG['SUB_DIR']+'/embed_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15ee3d3-8d30-4a22-80d9-1422324d9a1c",
   "metadata": {},
   "source": [
    "## 리더보드 제출을 위한 ZIP 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95731296-e440-4384-882d-2639237351b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 압축 완료: ./submission.zip\n"
     ]
    }
   ],
   "source": [
    "# 최종 제출물 (ZIP) 생성 경로\n",
    "# 제출물 (ZIP) 내에는 디렉토리(폴더)가 없이 구성해야합니다.\n",
    "zip_path = './submission.zip'\n",
    "\n",
    "# zip 파일 생성\n",
    "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for file_name in os.listdir(CFG['SUB_DIR']):\n",
    "        file_path = os.path.join(CFG['SUB_DIR'], file_name)\n",
    "\n",
    "        # 일반 파일이며 숨김 파일이 아닌 경우만 포함\n",
    "        if os.path.isfile(file_path) and not file_name.startswith('.'):\n",
    "            zipf.write(file_path, arcname=file_name)\n",
    "\n",
    "print(f\"✅ 압축 완료: {zip_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "color",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
